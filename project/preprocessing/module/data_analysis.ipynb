{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import sys \n",
    "sys.path.append(\"/net/data.isilon/ag-cherrmann/nschmidt/project/parse_xml_for_VAE/xml_data/\")\n",
    "sys.path.append(\"/net/data.isilon/ag-cherrmann/nschmidt/project/parse_xml_for_VAE/xml_data_t/\")\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_xml = \"/net/data.isilon/ag-cherrmann/nschmidt/project/parse_xml_for_VAE/xml_data/\"\n",
    "path_to_xml_t = \"/net/data.isilon/ag-cherrmann/nschmidt/project/parse_xml_for_VAE/xml_data_t/\"\n",
    "\n",
    "paths = list(pathlib.Path(path_to_xml).glob(\"*.csv\"))\n",
    "paths_t = list(pathlib.Path(path_to_xml_t).glob(\"*.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(paths: list, measurement: str):\n",
    "    total_fig = make_subplots(rows=len(paths), cols=1)\n",
    "    total_fig_data = []\n",
    "\n",
    "    for path in paths: \n",
    "        df = pd.read_csv(path, index_col=[0])\n",
    "        df = df.loc[df['Volume'] == measurement]\n",
    "        rois = df.columns[1:]   \n",
    "        \n",
    "        fig = go.Figure()\n",
    "\n",
    "        for roi in rois:\n",
    "            fig.add_trace(go.Box(y=df[roi], name=roi))\n",
    "\n",
    "        fig.update_layout(boxmode='group') #to group boxes of the same type.\n",
    "        fig.update_traces(boxpoints='all', jitter=.3)\n",
    "        \n",
    "        total_fig_data.append(fig)\n",
    "    \n",
    "    for idx, fig in enumerate(total_fig_data):\n",
    "        for trace in fig.data:\n",
    "            total_fig.add_trace(trace, row=idx+1, col=1)\n",
    "    \n",
    "    total_fig.update_layout(height=1200)\n",
    "    total_fig.show()\n",
    "\n",
    "plot_distribution(paths_t, measurement=\"Vgm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/net/data.isilon/ag-cherrmann/nschmidt/project/parse_xml_for_VAE/xml_data_t/Aggregated_suit_t.csv\"\n",
    "\n",
    "df = pd.read_csv(path, index_col=[0])\n",
    "rois = df.columns[2:]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=len(rois))\n",
    "for i, roi in enumerate(rois):\n",
    "    fig.add_trace(\n",
    "    go.Box(y=df[roi], name=roi), row=1, col=i+1)\n",
    "\n",
    "fig.update_traces(boxpoints='all', jitter=.3)\n",
    "fig.update_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_df(df: pd.DataFrame) -> dict: \n",
    "    data_dict = {}\n",
    "    rois = df.columns[1:]\n",
    "    for roi in rois:\n",
    "        roi_data = df[roi]\n",
    "        \n",
    "        data_dict[roi] = {\n",
    "            \"mean\":roi_data.mean(),\n",
    "            \"median\":roi_data.median(),\n",
    "            \"quantiles\": roi_data.quantile(q=[0.25, 0.5, 0.75]).to_list(),\n",
    "            \"min\": roi_data.min(), \n",
    "            \"max\": roi_data.max(),\n",
    "            \"var\": roi_data.var()\n",
    "        }\n",
    "        \n",
    "    return data_dict\n",
    "\n",
    "def analyze_features(paths: list):\n",
    "    all_data = {}\n",
    "    for path in paths_t: \n",
    "        path_str = str(path)\n",
    "        df = pd.read_csv(path, index_col=[0]).copy()\n",
    "        all_data[path_str] = {}\n",
    "\n",
    "        for measurement in [\"Vgm\", \"Vwm\"]:\n",
    "            df_m = df.loc[df['Volume'] == measurement].copy()\n",
    "            \n",
    "            if df_m.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            data_dict = analyze_df(df_m)\n",
    "            all_data[path_str][measurement] = data_dict\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = analyze_features(paths=paths_t)\n",
    "\n",
    "atlas = \"suit\"\n",
    "measurement = \"Vgm\"\n",
    "\n",
    "path = f\"/net/data.isilon/ag-cherrmann/nschmidt/project/parse_xml_for_VAE/xml_data_t/Aggregated_{atlas}_t.csv\"\n",
    "\n",
    "rois = [roi for roi in all_data[path][measurement]]\n",
    "variances = [all_data[path][measurement][roi][\"var\"] for roi in rois]\n",
    "\n",
    "sort_index = np.argsort(variances)\n",
    "sorted_rois = np.array(rois)[sort_index].tolist()\n",
    "print(f\"Variances: {variances}\")\n",
    "print(f\"ROIs: {rois}\")\n",
    "print(f\"Sorted ROIs (ascending): {sorted_rois}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "def normalize_and_scale_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Normalizes the columns (patient volumes) by Min-Max Scaling and scales the rows (ROIs) with Z-transformation.\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    column_sums = df_copy.sum()\n",
    "    \n",
    "    # Apply the formula: ln((10000*value)/sum_values + 1) \"Log transformation\"\n",
    "    # Alternatively for Min-Max Scaling: df_copy/df_copy.max() - Problem: Some rows have std = 0\n",
    "    transformed_df = np.log((10000 * df_copy) / column_sums + 1)\n",
    "    \n",
    "    norm_copy = transformed_df.copy()\n",
    "\n",
    "    cols = norm_copy.columns.get_level_values(-1).tolist()\n",
    "    unique_cols = list(set(cols))\n",
    "    \n",
    "    if len(unique_cols) > 0:\n",
    "        for col_type in unique_cols:\n",
    "            cols_to_scale = [col for col in norm_copy.columns if col[-1] == col_type] \n",
    "            print(cols_to_scale)\n",
    "            print(norm_copy[cols_to_scale])\n",
    "            print(norm_copy.apply(lambda x: pd.Series((x-x.mean())/x.std()) if x.std() > 0 else pd.Series([0]*len(x)), axis=\"columns\").head)\n",
    "    # else:\n",
    "    #     norm_copy = norm_copy.apply(z_scale, axis=\"columns\")\n",
    "    return norm_copy\n",
    "\n",
    "def z_scale(row):\n",
    "    sd = row.std()\n",
    "    mean = row.mean()\n",
    "    if sd > 0: \n",
    "        return pd.Series((row-mean)/sd)\n",
    "    else:\n",
    "        return pd.Series([0]*len(row))\n",
    "\n",
    "\n",
    "\n",
    "path_to_csv = \"/net/data.isilon/ag-cherrmann/nschmidt/project/parse_xml_for_VAE/xml_data/Aggregated_thalamic_nuclei.csv\"\n",
    "\n",
    "df = pd.read_csv(path_to_csv, header=[0, 1], index_col=0)\n",
    "\n",
    "df_norm = normalize_and_scale_df(df=df)\n",
    "df_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((5, 4))\n",
    "columns = pd.MultiIndex.from_tuples([('A', 'val1'), ('B', 'val1'), ('C', 'val1'), ('D', 'val1')], names = [\"Filename\", \"Volume\"])\n",
    "df_multi = pd.DataFrame(data, columns=columns)\n",
    "df_multi.index = [[\"a\", \"b\", \"c\", \"d\", \"e\"]]\n",
    "\n",
    "# Slicing a DataFrame with MultiIndex Header\n",
    "\n",
    "print(df_multi)\n",
    "cols = df_multi.columns.get_level_values(-1).tolist()\n",
    "unique_cols = list(set(cols))\n",
    "cols_to_scale = [col for col in df_multi.columns if col[-1] == col_type] \n",
    "\n",
    "\n",
    "df_multi[[(\"A\",\"val1\"), (\"B\",\"val1\")]] = df_multi[[(\"A\",\"val1\"), (\"B\",\"val1\")]].apply(lambda x: pd.Series((x-x.mean())/x.std()) if x.std() > 0 else pd.Series([0.0]*len(x)), axis=\"columns\")\n",
    "df_multi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA - per atlas / split for volume kind (vgm/vwm/csf):\n",
    "\n",
    "#general PCA function for one csv:\n",
    "def PCA_plot_Aggregated_data(csv_path: str, n_components: int = 2, fillna_value: float = 0.0, n_clusters: int = 3):\n",
    "    \n",
    "    df = pd.read_csv(csv_path, header=0, index_col=[0, 1])  # !! -> transposed csv\n",
    "\n",
    "    #amount volume categories -> plot size\n",
    "    volume_kinds = df.index.get_level_values('Volume').unique()\n",
    "    n = len(volume_kinds)\n",
    "    cols = 3 \n",
    "    rows = int(np.ceil(n / cols)) \n",
    "    # Erstelle Subplots basierend auf der Anzahl der Volume-Typen\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    #PCA per volume kind\n",
    "    for idx, volume_kind in enumerate(volume_kinds):\n",
    "    \n",
    "        df_volume = df.loc[df.index.get_level_values('Volume') == volume_kind]\n",
    "        df_volume.index = [f\"{patient}/{tissue}\" for patient, tissue in df_volume.index]\n",
    "        # PCA\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca_result = pca.fit_transform(df_volume)\n",
    "        # KMeans\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        labels = kmeans.fit_predict(pca_result)\n",
    "        # Plotting\n",
    "        ax = axes[idx]\n",
    "        scatter = ax.scatter(pca_result[:, 0], pca_result[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "        ax.set_title(f\"{os.path.basename(csv_path).replace('_t.csv', '').replace('Aggregated_','')} / {volume_kind}\")\n",
    "        ax.set_xlabel(\"PC1\")\n",
    "        ax.set_ylabel(\"PC2\")\n",
    "        cbar = plt.colorbar(scatter, ax=ax)\n",
    "        cbar.set_label('Cluster')\n",
    "\n",
    "    for i in range(n, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig, axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA function application to all the atlases -> one fig for each\n",
    "def apply_pca_to_all_csv_in_folder(folder_path: str, n_components: int = 2, fillna_value: float = 0.0, n_clusters: int = 3):\n",
    "   \n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    for idx, csv_file in enumerate(csv_files):\n",
    "        csv_path = os.path.join(folder_path, csv_file)\n",
    "        \n",
    "        PCA_plot_Aggregated_data(csv_path, n_components, fillna_value, n_clusters)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple correlation plots ROIS against ea\n",
    "\n",
    "def correlation_heatmap_Aggregated_data(csv_path: str, fillna_value: float = 0.0):\n",
    "    df = pd.read_csv(csv_path, header=0, index_col=[0, 1])  # !! -> transposed csv\n",
    "\n",
    "    #separate Vwm/Wgm/csf\n",
    "    volume_kinds = df.index.get_level_values('Volume').unique()\n",
    "    n = len(volume_kinds)\n",
    "    cols = 3\n",
    "    rows = int(np.ceil(n / cols))  \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    #correlation per volume category\n",
    "    for idx, volume_kind in enumerate(volume_kinds):\n",
    "        df_volume = df.loc[df.index.get_level_values('Volume') == volume_kind]\n",
    "\n",
    "        correlation_matrix = df_volume.corr()\n",
    "\n",
    "        ax = axes[idx]\n",
    "        sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1, center=0, square=True, ax=ax)\n",
    "        ax.set_title(f\"{os.path.basename(csv_path).replace('_t.csv', '').replace('Aggregated_','')} / {volume_kind}\", fontsize=10)\n",
    "        ax.set_xlabel(\"Features\", fontsize=8)\n",
    "        ax.set_ylabel(\"Features\", fontsize=8)\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "\n",
    "    for i in range(n, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "# application to all atlases\n",
    "def apply_correlation_heatmap(folder_path: str, fillna_value: float = 0.0):\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(folder_path, csv_file)\n",
    "        correlation_heatmap_Aggregated_data(csv_path, fillna_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_correlation_heatmap(\"/net/data.isilon/ag-cherrmann/nschmidt/project/parse_xml_for_VAE/xml_data_t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#numerical correlation comparison\n",
    "def correlation_Aggregated_data(csv_path: str, fillna_value: float = 0.0):\n",
    "    df = pd.read_csv(csv_path, header=0, index_col=[0, 1])  \n",
    "    volume_kinds = df.index.get_level_values('Volume').unique()\n",
    "    correlation_values_dict = {}\n",
    "\n",
    "    for idx, volume_kind in enumerate(volume_kinds):\n",
    "        df_volume = df.loc[df.index.get_level_values('Volume') == volume_kind]\n",
    "        correlation_matrix = df_volume.corr() \n",
    "        correlation_matrix_name = f\"{os.path.basename(csv_path).replace('_t.csv', '').replace('Aggregated_','')}_{volume_kind}_correlation_matrix\"\n",
    "        correlation_values_dict[correlation_matrix_name] = correlation_matrix \n",
    "    \n",
    "    return correlation_values_dict\n",
    "\n",
    "def get_top_correlations(correlation_matrix, top_n=5):\n",
    "    upper_tri = correlation_matrix.where(\n",
    "        np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    sorted_corrs = upper_tri.unstack().dropna().sort_values()\n",
    "    smallest = sorted_corrs.head(top_n)\n",
    "    largest = sorted_corrs.tail(top_n)\n",
    "    return smallest, largest\n",
    "\n",
    "def print_top_correlations(dict_all_atlases, top_n=5):\n",
    "    for name, matrix in dict_all_atlases.items():\n",
    "        print(f\"\\nTop {top_n} Correlations in {name}:\")\n",
    "        smallest, largest = get_top_correlations(matrix, top_n)\n",
    "\n",
    "        print(\"\\n Smallest Correlations:\")\n",
    "        print(smallest)\n",
    "\n",
    "        print(\"\\n Largest Correlations (â‰ 1):\")\n",
    "        print(largest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"/net/data.isilon/ag-cherrmann/nschmidt/project/parse_xml_for_VAE/xml_data_t\"\n",
    "\n",
    "dict_all_atlases = {}\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        result = correlation_Aggregated_data(full_path)\n",
    "        dict_all_atlases.update(result)\n",
    "        \n",
    "print_top_correlations(dict_all_atlases, top_n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM: \n",
    "-> largest correlations are often due to respective areas in left & right hemisphere \n",
    "-> more interesting are areas that are not bilateral equivalent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation again !ignoring symmetrical pairs!\n",
    "import re\n",
    "def clean_roi_name(roi):\n",
    "    #removes l or r in name -> neutral name\n",
    "    return re.sub(r'^[lr]', '', roi)\n",
    "\n",
    "def get_top_correlations_filtered(correlation_matrix, top_n=5):\n",
    "    upper_tri = correlation_matrix.where(\n",
    "        np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    sorted_corrs = upper_tri.unstack().dropna().sort_values()\n",
    "\n",
    "    #only non-equal pairs\n",
    "    filtered_corrs = sorted_corrs[\n",
    "        [(clean_roi_name(i) != clean_roi_name(j)) for i, j in sorted_corrs.index]\n",
    "    ]\n",
    "\n",
    "    smallest = filtered_corrs.head(top_n)\n",
    "    largest = filtered_corrs.tail(top_n)\n",
    "\n",
    "    return smallest, largest\n",
    "\n",
    "def print_top_correlations_nosym(dict_all_atlases, top_n=5):\n",
    "    for name, matrix in dict_all_atlases.items():\n",
    "        print(f\"\\nðŸ“Š Top {top_n} Correlations in {name} (non bilateral):\")\n",
    "        smallest, largest = get_top_correlations_filtered(matrix, top_n)\n",
    "\n",
    "        print(\"\\nðŸ”» Smallest Correlations:\")\n",
    "        print(smallest)\n",
    "\n",
    "        print(\"\\nðŸ”º Largest Correlations:\")\n",
    "        print(largest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/net/data.isilon/ag-cherrmann/nschmidt/project/parse_xml_for_VAE/xml_data_t\"\n",
    "\n",
    "dict_all_atlases = {}\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        result = correlation_Aggregated_data(full_path)\n",
    "        dict_all_atlases.update(result)\n",
    "        \n",
    "print_top_correlations_nosym(dict_all_atlases, top_n=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LISA_ba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
